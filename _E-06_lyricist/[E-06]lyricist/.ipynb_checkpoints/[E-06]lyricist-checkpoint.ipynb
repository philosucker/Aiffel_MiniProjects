{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['', '', '[Spoken Intro:]', 'You ever want something ', \"that you know you shouldn't have \", \"The more you know you shouldn't have it, \", 'The more you want it ', 'And then one day you get it, ', \"It's so good too \", \"But it's just like my girl \", \"When she's around me \", 'I just feel so good, so good ', 'But right now I just feel cold, so cold ', 'Right down to my bones ', \"'Cause ooh... \", \"Ain't no sunshine when she's gone \", \"It's not warm when she's away \", \"Ain't no sunshine when she's gone \", \"And she's always gone too long \", 'Anytime she goes away ', '', \"Wonder this time where she's gone \", \"Wonder if she's gone to stay \", \"Ain't no sunshine when she's gone \", \"And this house just ain't no home \", 'Anytime she goes away ', '', 'I know, I know, I know, I know, ', 'I know, know, know, know, know, ', 'I know, I know, ', 'Hey I ought to leave ', 'I ought to leave her alone ', \"Ain't no sunshine when she's gone \", '', \"Ain't no sunshine when she's gone \", 'Only darkness every day ', \"Ain't no sunshine when she's gone \", \"And this house just ain't no home \", 'Anytime she goes away', '', '', 'Can it be I stayed away too long', 'Did I leave your mind when I was gone', \"It's not my thing trying to get back\", \"But this time let me tell you where I'm at\", \"You don't have to worry 'cause I'm coming\", 'Back to where I should have always stayed', \"And now I've heard the maybe to your story\", \"And it's enough love for me to stay\", '', 'Can it be I stayed away too long', 'Did I leave your mind when I was gone', \"It's not my thing trying to get back\", \"But this time let me tell you where I'm at\", '', '[Chorus:]', 'I wanna, wanna be where you are', 'Anywhere you are', 'I wanna, wanna be where you are', 'Everywhere you are', '', '[Bridge:]', \"Please don't close the door to our future\", \"There's so many things we haven't tried\", 'I could love you better than I used to', 'And give you all the love I have inside', '', '[Chorus:]', 'I wanna, wanna be where you are', 'Any, any, anywhere you are', 'I wanna, wanna be where you are', 'I gotta be where you are', '', '', 'Oooh', \"Foolish of me, I couldn't see \", 'The forest for the trees ', \"You've been so true \", \"I've been so cruel to you \", \"Oh, girl please don't leave me\", '', '[Chorus:]', \"If you stay I'll find a way \", 'To erase the past ', \"Baby don't leave me \", \"Girl don't take your love from me \", 'Oh girl ', '', 'What can I say to make you stay ', \"Baby, don't leave me \", \"You've been so kind \", 'It blows my mind to know', 'Oh girl, I really hurt you ', '', '[Chorus:]', \"Tell me girl, it's not to late \", 'And you give me a chance ', 'To make it up to you ', \"Don't take your love from me \", \"Girl don't take your love \"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf    # 대망의 텐서플로우!\n",
    "\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/mini_projects/_E-06_lyricist/song/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "        \n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정제 데이터 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> you ever want something <end>',\n",
       " '<start> that you know you shouldn t have <end>',\n",
       " '<start> the more you know you shouldn t have it , <end>',\n",
       " '<start> the more you want it <end>',\n",
       " '<start> and then one day you get it , <end>',\n",
       " '<start> it s so good too <end>',\n",
       " '<start> but it s just like my girl <end>',\n",
       " '<start> when she s around me <end>',\n",
       " '<start> i just feel so good , so good <end>',\n",
       " '<start> but right now i just feel cold , so cold <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    if sentence[0] == \"[\" : continue # 이거  추가하기 전엔 175749. \n",
    "    if sentence.count(\" \") > 10 : continue  # 이거 추가하기 전엔 175171\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150691"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토크나이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2   7 162 ...   0   0   0]\n",
      " [  2  17   7 ...   0   0   0]\n",
      " [  2   6  97 ...   0   0   0]\n",
      " ...\n",
      " [  2  43 922 ...   0   0   0]\n",
      " [  2  43  64 ...   0   0   0]\n",
      " [  2   8  82 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f9c1619ca50>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=20000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150691, 15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    7  162   65  195    3    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2   17    7   36    7 1667   15   76    3    0    0    0    0    0\n",
      "     0]\n",
      " [   2    6   97    7   36    7 1667   15   76   11    4    3    0    0\n",
      "     0]\n",
      " [   2    6   97    7   65   11    3    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2    8   98   60  118    7   45   11    4    3    0    0    0    0\n",
      "     0]\n",
      " [   2   11   16   31  110  101    3    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2   35   11   16   32   24   13   81    3    0    0    0    0    0\n",
      "     0]\n",
      " [   2   47   46   16  134   12    3    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2    5   32  104   31  110    4   31  110    3    0    0    0    0\n",
      "     0]\n",
      " [   2   35   86   52    5   32  104  343    4   31  343    3    0    0\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 소스  /  타겟 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2   7 162  65 195   3   0   0   0   0   0   0   0   0]\n",
      "[  7 162  65 195   3   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train / test set 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (120552, 14)\n",
      "Target Train: (120552, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                         tgt_input,\n",
    "                                                         test_size=0.2,\n",
    "                                                         shuffle=True)\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련 데이터 셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((512, 14), (512, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 512\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 생성 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512\n",
    "hidden_size = 3072\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(512, 14, 20001), dtype=float32, numpy=\n",
       "array([[[ 7.1678653e-05, -6.2685831e-05, -6.4311098e-06, ...,\n",
       "         -1.7581582e-04, -1.3023868e-04, -1.7823395e-04],\n",
       "        [ 2.1119753e-04, -5.4232140e-05,  1.3922836e-04, ...,\n",
       "         -3.2008902e-04, -3.0532060e-04,  4.5801236e-05],\n",
       "        [ 1.8744014e-04,  1.5029067e-04,  2.1425651e-04, ...,\n",
       "         -2.5212139e-04, -3.9736545e-04,  5.7020388e-06],\n",
       "        ...,\n",
       "        [ 3.8216100e-04, -2.9035631e-04,  4.8033631e-04, ...,\n",
       "         -2.0785474e-03,  6.0372782e-04, -1.2940018e-03],\n",
       "        [ 3.9879777e-04, -3.9511238e-04,  7.2299340e-04, ...,\n",
       "         -2.4178473e-03,  4.0005794e-04, -1.7941239e-03],\n",
       "        [ 4.7312368e-04, -5.5164046e-04,  9.4836898e-04, ...,\n",
       "         -2.7159462e-03,  1.4825485e-04, -2.2624708e-03]],\n",
       "\n",
       "       [[ 7.1678653e-05, -6.2685831e-05, -6.4311098e-06, ...,\n",
       "         -1.7581582e-04, -1.3023868e-04, -1.7823395e-04],\n",
       "        [ 4.2497658e-04, -2.3679255e-04, -2.5238024e-04, ...,\n",
       "         -1.2300345e-04, -2.3705409e-04, -3.9042140e-04],\n",
       "        [ 6.3276826e-04, -4.5169020e-04, -4.1801942e-04, ...,\n",
       "          3.0902307e-04, -2.7535309e-04, -5.9564819e-04],\n",
       "        ...,\n",
       "        [ 2.5547761e-04, -2.4672502e-04,  1.4274619e-03, ...,\n",
       "         -1.1514374e-03, -2.5601385e-04, -1.9602193e-03],\n",
       "        [ 4.2122172e-04, -4.5002604e-04,  1.6265579e-03, ...,\n",
       "         -1.5450040e-03, -4.3151967e-04, -2.3812165e-03],\n",
       "        [ 5.9043639e-04, -6.5454817e-04,  1.7994511e-03, ...,\n",
       "         -1.9089234e-03, -5.9440045e-04, -2.7718989e-03]],\n",
       "\n",
       "       [[ 7.1678653e-05, -6.2685831e-05, -6.4311098e-06, ...,\n",
       "         -1.7581582e-04, -1.3023868e-04, -1.7823395e-04],\n",
       "        [ 4.2572196e-06,  2.4747928e-06, -1.8207822e-04, ...,\n",
       "         -1.5455707e-04, -6.9763781e-05, -5.6859568e-05],\n",
       "        [-4.1980430e-04,  3.2222444e-05, -2.4277015e-04, ...,\n",
       "         -3.6922560e-04, -3.1022399e-05,  6.3201362e-05],\n",
       "        ...,\n",
       "        [ 6.6160595e-05,  1.3145892e-04,  4.0532721e-04, ...,\n",
       "         -1.2554338e-03,  3.4350433e-04, -2.2287045e-03],\n",
       "        [ 2.4701408e-04, -1.3536967e-04,  6.9180503e-04, ...,\n",
       "         -1.6157437e-03,  6.7771864e-05, -2.6469380e-03],\n",
       "        [ 4.2451039e-04, -3.9854931e-04,  9.6607569e-04, ...,\n",
       "         -1.9550177e-03, -1.8827115e-04, -3.0183801e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 7.1678653e-05, -6.2685831e-05, -6.4311098e-06, ...,\n",
       "         -1.7581582e-04, -1.3023868e-04, -1.7823395e-04],\n",
       "        [ 3.0692495e-04, -2.0432196e-04,  9.8227747e-06, ...,\n",
       "         -2.6361691e-04,  1.9102200e-04, -2.4989797e-04],\n",
       "        [ 4.8600096e-04, -4.3799620e-04,  2.1968967e-04, ...,\n",
       "         -2.4559366e-04,  7.0848252e-04, -3.5966269e-04],\n",
       "        ...,\n",
       "        [ 7.0213707e-04, -7.0382247e-04,  1.6474614e-03, ...,\n",
       "         -2.1962966e-03, -7.3391787e-04, -3.2269249e-03],\n",
       "        [ 8.2857447e-04, -8.5107388e-04,  1.8446204e-03, ...,\n",
       "         -2.4622388e-03, -8.9061476e-04, -3.5217756e-03],\n",
       "        [ 9.2847960e-04, -9.9819608e-04,  2.0183399e-03, ...,\n",
       "         -2.7190642e-03, -1.0164111e-03, -3.7851916e-03]],\n",
       "\n",
       "       [[ 7.1678653e-05, -6.2685831e-05, -6.4311098e-06, ...,\n",
       "         -1.7581582e-04, -1.3023868e-04, -1.7823395e-04],\n",
       "        [-6.4049789e-05,  2.8092907e-05,  9.9330064e-05, ...,\n",
       "         -2.7182020e-04, -2.7469639e-05, -6.7668079e-05],\n",
       "        [-2.6323652e-04, -5.9658814e-05, -1.0126124e-04, ...,\n",
       "         -3.7406225e-04, -1.9858303e-04, -9.2855553e-05],\n",
       "        ...,\n",
       "        [ 7.7706558e-04, -1.4522083e-03,  1.7366877e-03, ...,\n",
       "         -2.7724016e-03, -1.0759985e-03, -3.2971178e-03],\n",
       "        [ 9.0032414e-04, -1.5789473e-03,  1.9117243e-03, ...,\n",
       "         -3.0159024e-03, -1.1373197e-03, -3.5992756e-03],\n",
       "        [ 9.9339755e-04, -1.6872953e-03,  2.0656323e-03, ...,\n",
       "         -3.2454520e-03, -1.1862785e-03, -3.8712157e-03]],\n",
       "\n",
       "       [[ 7.1678653e-05, -6.2685831e-05, -6.4311098e-06, ...,\n",
       "         -1.7581582e-04, -1.3023868e-04, -1.7823395e-04],\n",
       "        [ 3.0021335e-04, -3.0257823e-05,  5.3095984e-05, ...,\n",
       "         -1.5205443e-04, -4.0711011e-04, -3.7680820e-04],\n",
       "        [ 4.3307204e-04,  5.2679065e-05,  1.4707776e-04, ...,\n",
       "          3.3428852e-05, -2.2189174e-04, -4.6691936e-04],\n",
       "        ...,\n",
       "        [ 1.0602921e-03,  6.1845238e-04,  7.7240157e-04, ...,\n",
       "         -2.3627284e-03, -2.0097962e-04, -1.7715597e-03],\n",
       "        [ 1.1721370e-03,  2.6228849e-04,  9.7106246e-04, ...,\n",
       "         -2.6383460e-03, -3.3114493e-04, -2.2725377e-03],\n",
       "        [ 1.2860089e-03, -8.3247731e-05,  1.1583562e-03, ...,\n",
       "         -2.8643135e-03, -4.6498294e-04, -2.7313845e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  10240512  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  44052480  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  75509760  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  61463073  \n",
      "=================================================================\n",
      "Total params: 191,265,825\n",
      "Trainable params: 191,265,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "235/235 [==============================] - 416s 2s/step - loss: 3.5686\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 409s 2s/step - loss: 2.9498\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 409s 2s/step - loss: 2.7010\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 409s 2s/step - loss: 2.4855\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 410s 2s/step - loss: 2.2754\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 410s 2s/step - loss: 2.0758\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 410s 2s/step - loss: 1.8839\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 410s 2s/step - loss: 1.6999\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 410s 2s/step - loss: 1.5269\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 410s 2s/step - loss: 1.3687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c1345ced0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트 데이터 셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((512, 14), (512, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_val)\n",
    "BATCH_SIZE = 512\n",
    "steps_per_epoch = len(enc_val) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 20000개와, 여기 포함되지 않은 0:<pad>를 포함하여 20001개\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트 셋 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 32s 558ms/step - loss: 2.2170\n"
     ]
    }
   ],
   "source": [
    "results=model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장 생성기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=15):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you know i m bad , i m bad you know it <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트 진행 과정 및 결과   \n",
    "   \n",
    "1. 전체 토큰 수를 줄이기 위해서 전처리 시 if sentence.count(\" \") > 10 : continue 이러한 코드를 사용했다.   \n",
    "   \n",
    "   \n",
    "2. 전체적으로 무난했다. 진행이 막혀 크게 어려움을 겪은 구간이 없었다. (그래서 한편으론 두렵다. 잘한게 맞나?)   \n",
    "   \n",
    "   \n",
    "   \n",
    "**1차 시기** : batch size = 256, hidden size = 2048 , embedding size = 512 ----> val loss = 2.31   \n",
    "\n",
    "**2차 시기** : batch size = 512, hidden_size = 3072 , embedding_size = 512 ----> val loss = 2.21   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트 진행 중 의문점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 정규표현식으로 만들어보고 싶은데 좀 더 공부가 필요하다. **공부할 것**   \n",
    "   \n",
    "\n",
    "2. 데이터셋 생성 코드가 아직 잘 이해되지 않는다. steps_per_epoch, tf.data.Dataset.from_tensor_slices 의 역할이 아직 분명히 와닿지 않는다   \n",
    "훈련셋, 테스트셋을 분리하는 것과 데이터셋 생성의 차이가 무엇인가? **공부할 것**   \n",
    "   \n",
    "3. 문장생성기 코드도 완벽하게 파악이 안 됐다. **공부할 것**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회고   \n",
    "   \n",
    "매일 매일 다른 이유로 스트레스를 받고 있었다.   \n",
    "주말엔 풀잎 머신러닝 강의가 이해되지 않아서,   \n",
    "그저께는 복습할 게 너무 밀려 있었서,   \n",
    "어제는 오늘 노드가 이해가지 않아서,   \n",
    "오늘은 프로젝트에 진전이 없어서,   \n",
    "지금 하고 있는 모든 것들을 하나하나 이해하면서 앞으로 나아가기 보다   \n",
    "일단 되게 하고 나중에 생각해보는 방식으로 진행하게 되면서 오는 불만족도 큰 몫을 차지했다.   \n",
    "   \n",
    "그 중에서도 가장 날 힘들게 했던 건 프로젝트였다.   \n",
    "공식 LMS가 끝나고 지시딥 스터디를 마치자마자 저녁을 먹고   \n",
    "곧바로 새벽 두시 세시까지 뭔가를 계속 하지만 머리 속에 남는 느낌이 들지 않았다.   \n",
    "accuracy 몇%, loss몇% 한 두 줄이 뜨는 걸 보기 위해 겪는 시행착오들에서 뭔가를 배운다는 느낌보단 (물론 배우는 게 있긴 하지만)   \n",
    "결국 그냥 뭔갈 '되게' 하고 말 뿐, 그것이 어떻게 왜 그렇게 되는지에 대한 이해는 한 켠에 제쳐져있다는 불쾌감이 더 컸다.   \n",
    "   \n",
    "일단 끝까지 가보는 것 역시 나쁜 방법은 아니다.   \n",
    "그러나 지금은 선을 넘었다.   \n",
    "바닥이 있는 데서 위로 올라가다 떨어지면, 팔다리 하나가 부러질지언정 바닥을 딛고 다시 올라갈 수 있다.   \n",
    "허나 바닥이 없이 위로 올라가다 떨어지면, 죽기 전까지 계속 떨어지기만 할 뿐이다.   \n",
    "좀 뒤쳐져도 어쩔 수 없다.   \n",
    "그동안 쌓아둔 걸 해치우고 넘어가지 않고선 안되겠다.   \n",
    "\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
