{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sensitive-tennessee",
   "metadata": {},
   "source": [
    "# 프로젝트 목차   \n",
    "\n",
    "## 1. num_words 실험 1)None,  2)5000,  3)3000, 4)4000, 5)8000   \n",
    "\n",
    "### (1)data load      \n",
    "### (2) data decode      \n",
    "### (3) vectorize      \n",
    "### (4) 8개 모델 테스트 및 평가 ( NB, CNV, LR, SVM, DT, RF, GBT, Ensemble)   \n",
    "   \n",
    "   \n",
    "## 2. 딥러닝 모델과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "indie-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB #다항분포 나이브 베이즈 모델\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score #정확도 계산\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-tuner",
   "metadata": {},
   "source": [
    "# 1. num_words 실험 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-emperor",
   "metadata": {},
   "source": [
    "## 1) num_words = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-alignment",
   "metadata": {},
   "source": [
    "### (1) data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reliable-dining",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-information",
   "metadata": {},
   "source": [
    "### (2) data decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "determined-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "\n",
    "index_to_word = {index + 3 : word for word, index in word_index.items()}\n",
    "\n",
    "# index_to_word에 숫자 0은 <pad>, 숫자 1은 <sos>, 숫자 2는 <unk>를 넣어줍니다.\n",
    "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    index_to_word[index]=token\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(x_train)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train = decoded\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(x_test)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test = decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-belly",
   "metadata": {},
   "source": [
    "### (3) vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "owned-attack",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtmvector = CountVectorizer()\n",
    "x_train_dtm = dtmvector.fit_transform(x_train)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidfv = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "\n",
    "x_test_dtm = dtmvector.transform(x_test) #테스트 데이터를 DTM으로 변환\n",
    "tfidfv_test = tfidf_transformer.transform(x_test_dtm) #DTM을 TF-IDF 행렬로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-bryan",
   "metadata": {},
   "source": [
    "### (4) 8개 모델 테스트 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-apollo",
   "metadata": {},
   "source": [
    "###  NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "later-remark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.5997328584149599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.79      0.21      0.33       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.72      0.92      0.81       813\n",
      "           4       0.45      0.96      0.61       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.00      0.00      0.00        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.80      0.29      0.42        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.75      0.18      0.29        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.73      0.58      0.64       133\n",
      "          20       0.00      0.00      0.00        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       0.00      0.00      0.00        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.60      2246\n",
      "   macro avg       0.09      0.07      0.07      2246\n",
      "weighted avg       0.50      0.60      0.50      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "mod = MultinomialNB()\n",
    "mod.fit(tfidfv, y_train)\n",
    "predicted = mod.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, mod.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-poverty",
   "metadata": {},
   "source": [
    "### CNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "diverse-moderator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7649154051647373\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.50      0.63        12\n",
      "           1       0.63      0.88      0.73       105\n",
      "           2       0.91      0.50      0.65        20\n",
      "           3       0.87      0.91      0.89       813\n",
      "           4       0.75      0.93      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.43      0.08      0.13        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.96      0.73      0.83        30\n",
      "          11       0.55      0.67      0.61        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.62      0.54      0.58        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.67      0.77      0.71        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.65      0.55      0.59        20\n",
      "          19       0.55      0.80      0.65       133\n",
      "          20       0.89      0.23      0.36        70\n",
      "          21       0.84      0.59      0.70        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.71      0.42      0.53        12\n",
      "          24       0.50      0.11      0.17        19\n",
      "          25       0.83      0.61      0.70        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.33      0.10      0.15        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.31      0.47        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.76      2246\n",
      "   macro avg       0.62      0.42      0.46      2246\n",
      "weighted avg       0.75      0.76      0.73      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cb = ComplementNB()\n",
    "cb.fit(tfidfv, y_train)\n",
    "predicted = cb.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, cb.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-franklin",
   "metadata": {},
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "opening-average",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.813446126447017\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.75      0.80      0.77       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.93      0.93      0.93       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.93      1.00      0.97        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.68      0.71      0.69        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.66      0.73      0.70        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.61      0.62      0.61        37\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       0.71      0.56      0.63         9\n",
      "          16       0.71      0.77      0.74        99\n",
      "          17       0.67      0.50      0.57        12\n",
      "          18       0.76      0.65      0.70        20\n",
      "          19       0.69      0.70      0.69       133\n",
      "          20       0.60      0.49      0.54        70\n",
      "          21       0.63      0.81      0.71        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.69      0.75      0.72        12\n",
      "          24       0.62      0.53      0.57        19\n",
      "          25       0.92      0.74      0.82        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.75      0.30      0.43        10\n",
      "          29       0.57      1.00      0.73         4\n",
      "          30       0.89      0.67      0.76        12\n",
      "          31       0.75      0.46      0.57        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.29      0.44         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.42      0.45      0.43        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.50      0.40      0.44         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.83      0.62      0.71         8\n",
      "          42       1.00      1.00      1.00         3\n",
      "          43       0.86      1.00      0.92         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.76      0.64      0.67      2246\n",
      "weighted avg       0.81      0.81      0.81      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=10000, penalty='l2')\n",
    "lr.fit(tfidfv, y_train)\n",
    "predicted = lr.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, lr.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-singapore",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "intelligent-exemption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7773820124666073\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.67      0.70        12\n",
      "           1       0.69      0.70      0.69       105\n",
      "           2       0.64      0.70      0.67        20\n",
      "           3       0.91      0.91      0.91       813\n",
      "           4       0.81      0.85      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.87      0.93      0.90        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.63      0.71      0.67        38\n",
      "           9       1.00      0.84      0.91        25\n",
      "          10       0.89      0.83      0.86        30\n",
      "          11       0.63      0.75      0.69        83\n",
      "          12       0.27      0.23      0.25        13\n",
      "          13       0.56      0.51      0.54        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.33      0.11      0.17         9\n",
      "          16       0.61      0.72      0.66        99\n",
      "          17       1.00      0.25      0.40        12\n",
      "          18       0.92      0.55      0.69        20\n",
      "          19       0.66      0.65      0.65       133\n",
      "          20       0.49      0.44      0.47        70\n",
      "          21       0.59      0.81      0.69        27\n",
      "          22       0.50      0.29      0.36         7\n",
      "          23       0.57      0.67      0.62        12\n",
      "          24       0.69      0.47      0.56        19\n",
      "          25       0.85      0.71      0.77        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.50      0.67         4\n",
      "          28       0.33      0.30      0.32        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       0.83      0.42      0.56        12\n",
      "          31       0.86      0.46      0.60        13\n",
      "          32       0.78      0.70      0.74        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.57      0.57      0.57         7\n",
      "          35       0.67      0.33      0.44         6\n",
      "          36       0.33      0.27      0.30        11\n",
      "          37       0.33      0.50      0.40         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.50      0.40      0.44         5\n",
      "          40       0.33      0.20      0.25        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       0.67      0.67      0.67         3\n",
      "          43       0.60      1.00      0.75         6\n",
      "          44       0.57      0.80      0.67         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.78      2246\n",
      "   macro avg       0.63      0.56      0.58      2246\n",
      "weighted avg       0.78      0.78      0.77      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
    "lsvc.fit(tfidfv, y_train)\n",
    "predicted = lsvc.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, lsvc.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-airline",
   "metadata": {},
   "source": [
    "### DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "extraordinary-price",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6211041852181657\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.69      0.43      0.53       105\n",
      "           2       0.75      0.45      0.56        20\n",
      "           3       0.94      0.85      0.89       813\n",
      "           4       0.40      0.89      0.55       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.16      0.28        25\n",
      "          10       0.89      0.80      0.84        30\n",
      "          11       0.58      0.60      0.59        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.61      0.83      0.70        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.67      0.41      0.50       133\n",
      "          20       0.83      0.07      0.13        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       0.60      0.19      0.29        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      2246\n",
      "   macro avg       0.22      0.15      0.15      2246\n",
      "weighted avg       0.62      0.62      0.58      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "tree.fit(tfidfv, y_train)\n",
    "predicted = tree.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, tree.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-concept",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "social-technical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6544968833481746\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.58      0.35        12\n",
      "           1       0.35      0.60      0.44       105\n",
      "           2       0.32      0.40      0.36        20\n",
      "           3       0.82      0.89      0.85       813\n",
      "           4       0.62      0.84      0.71       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.67      0.43      0.52        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.51      0.47      0.49        38\n",
      "           9       1.00      0.28      0.44        25\n",
      "          10       0.46      0.20      0.28        30\n",
      "          11       0.56      0.64      0.60        83\n",
      "          12       0.40      0.15      0.22        13\n",
      "          13       0.33      0.16      0.22        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.59      0.46      0.52        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.44      0.20      0.28        20\n",
      "          19       0.61      0.50      0.55       133\n",
      "          20       0.51      0.33      0.40        70\n",
      "          21       0.55      0.22      0.32        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.33      0.08      0.13        12\n",
      "          24       0.33      0.05      0.09        19\n",
      "          25       1.00      0.23      0.37        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.10      0.18        10\n",
      "          33       1.00      0.40      0.57         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.43      0.27      0.33        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.75      0.50      0.60         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.65      2246\n",
      "   macro avg       0.40      0.25      0.28      2246\n",
      "weighted avg       0.63      0.65      0.62      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "forest.fit(tfidfv, y_train)\n",
    "predicted = forest.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, forest.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-scholarship",
   "metadata": {},
   "source": [
    "### GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "steady-evans",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7702582368655387\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.50      0.55        12\n",
      "           1       0.81      0.71      0.76       105\n",
      "           2       0.58      0.70      0.64        20\n",
      "           3       0.87      0.91      0.89       813\n",
      "           4       0.78      0.86      0.82       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.77      0.71      0.74        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.60      0.63      0.62        38\n",
      "           9       0.91      0.80      0.85        25\n",
      "          10       0.79      0.77      0.78        30\n",
      "          11       0.61      0.65      0.63        83\n",
      "          12       0.50      0.46      0.48        13\n",
      "          13       0.48      0.32      0.39        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.25      0.11      0.15         9\n",
      "          16       0.72      0.71      0.71        99\n",
      "          17       0.83      0.42      0.56        12\n",
      "          18       0.59      0.50      0.54        20\n",
      "          19       0.71      0.64      0.67       133\n",
      "          20       0.64      0.41      0.50        70\n",
      "          21       0.61      0.63      0.62        27\n",
      "          22       0.33      0.14      0.20         7\n",
      "          23       0.62      0.67      0.64        12\n",
      "          24       0.69      0.47      0.56        19\n",
      "          25       0.83      0.65      0.73        31\n",
      "          26       1.00      1.00      1.00         8\n",
      "          27       0.33      0.50      0.40         4\n",
      "          28       0.25      0.20      0.22        10\n",
      "          29       0.43      0.75      0.55         4\n",
      "          30       0.36      0.42      0.38        12\n",
      "          31       0.50      0.54      0.52        13\n",
      "          32       1.00      1.00      1.00        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.60      0.43      0.50         7\n",
      "          35       0.33      0.17      0.22         6\n",
      "          36       0.50      0.64      0.56        11\n",
      "          37       0.50      1.00      0.67         2\n",
      "          38       0.33      0.33      0.33         3\n",
      "          39       0.33      0.20      0.25         5\n",
      "          40       0.83      0.50      0.62        10\n",
      "          41       0.62      0.62      0.62         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.43      0.50      0.46         6\n",
      "          44       0.80      0.80      0.80         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.62      0.57      0.57      2246\n",
      "weighted avg       0.77      0.77      0.76      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grbt = GradientBoostingClassifier(random_state=0) # verbose=3\n",
    "grbt.fit(tfidfv, y_train)\n",
    "predicted = grbt.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, grbt.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-board",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "processed-cowboy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.8187889581478184\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75        12\n",
      "           1       0.80      0.77      0.79       105\n",
      "           2       0.67      0.80      0.73        20\n",
      "           3       0.93      0.94      0.93       813\n",
      "           4       0.82      0.88      0.85       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.87      0.93      0.90        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.69      0.71      0.70        38\n",
      "           9       0.80      0.80      0.80        25\n",
      "          10       0.90      0.90      0.90        30\n",
      "          11       0.67      0.71      0.69        83\n",
      "          12       0.60      0.46      0.52        13\n",
      "          13       0.69      0.65      0.67        37\n",
      "          14       0.29      1.00      0.44         2\n",
      "          15       0.40      0.22      0.29         9\n",
      "          16       0.73      0.76      0.74        99\n",
      "          17       0.75      0.50      0.60        12\n",
      "          18       0.73      0.55      0.63        20\n",
      "          19       0.71      0.71      0.71       133\n",
      "          20       0.66      0.50      0.57        70\n",
      "          21       0.63      0.81      0.71        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.62      0.67      0.64        12\n",
      "          24       0.73      0.58      0.65        19\n",
      "          25       0.92      0.77      0.84        31\n",
      "          26       1.00      1.00      1.00         8\n",
      "          27       0.67      0.50      0.57         4\n",
      "          28       0.33      0.30      0.32        10\n",
      "          29       0.50      1.00      0.67         4\n",
      "          30       0.54      0.58      0.56        12\n",
      "          31       0.82      0.69      0.75        13\n",
      "          32       1.00      1.00      1.00        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.80      0.57      0.67         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.54      0.64      0.58        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       1.00      0.40      0.57        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       1.00      1.00      1.00         3\n",
      "          43       0.83      0.83      0.83         6\n",
      "          44       0.80      0.80      0.80         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.82      2246\n",
      "   macro avg       0.73      0.66      0.66      2246\n",
      "weighted avg       0.82      0.82      0.81      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voting_classifier = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "], voting='soft', n_jobs=-1)\n",
    "voting_classifier.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = voting_classifier.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "\n",
    "print(classification_report(y_test, voting_classifier.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-league",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "thirty-senegal",
   "metadata": {},
   "source": [
    "# 2) num_words = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-amplifier",
   "metadata": {},
   "source": [
    "# data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "silent-fashion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=5000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-radius",
   "metadata": {},
   "source": [
    "# data decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hybrid-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "\n",
    "index_to_word = {index + 3 : word for word, index in word_index.items()}\n",
    "\n",
    "# index_to_word에 숫자 0은 <pad>, 숫자 1은 <sos>, 숫자 2는 <unk>를 넣어줍니다.\n",
    "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    index_to_word[index]=token\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(x_train)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train = decoded\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(x_test)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test = decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-bradford",
   "metadata": {},
   "source": [
    "# vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "angry-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtmvector = CountVectorizer()\n",
    "x_train_dtm = dtmvector.fit_transform(x_train)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidfv = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "\n",
    "x_test_dtm = dtmvector.transform(x_test) #테스트 데이터를 DTM으로 변환\n",
    "tfidfv_test = tfidf_transformer.transform(x_test_dtm) #DTM을 TF-IDF 행렬로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-missouri",
   "metadata": {},
   "source": [
    "# 8개 모델 테스트 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-miami",
   "metadata": {},
   "source": [
    "###  NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "crazy-volunteer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6731967943009796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.50      0.80      0.62       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.86      0.89      0.87       813\n",
      "           4       0.59      0.95      0.73       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.28      0.44        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.48      0.73      0.58        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       1.00      0.14      0.24        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.60      0.66      0.62        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.51      0.81      0.63       133\n",
      "          20       0.90      0.13      0.23        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.06      0.12        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.67      2246\n",
      "   macro avg       0.16      0.12      0.11      2246\n",
      "weighted avg       0.60      0.67      0.60      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "mod = MultinomialNB()\n",
    "mod.fit(tfidfv, y_train)\n",
    "predicted = mod.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, mod.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-variable",
   "metadata": {},
   "source": [
    "### CNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mobile-frederick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7707034728406055\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.63      0.86      0.73       105\n",
      "           2       0.91      0.50      0.65        20\n",
      "           3       0.91      0.89      0.90       813\n",
      "           4       0.74      0.92      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.86      0.86      0.86        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.57      0.21      0.31        38\n",
      "           9       0.82      0.92      0.87        25\n",
      "          10       0.96      0.80      0.87        30\n",
      "          11       0.54      0.76      0.63        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.69      0.59      0.64        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.67      0.79      0.72        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.55      0.60      0.57        20\n",
      "          19       0.56      0.80      0.66       133\n",
      "          20       0.79      0.33      0.46        70\n",
      "          21       0.78      0.67      0.72        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.67      0.33      0.44        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       0.86      0.77      0.81        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.33      0.20      0.25        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.15      0.27        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.63      0.44      0.48      2246\n",
      "weighted avg       0.76      0.77      0.75      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cb = ComplementNB()\n",
    "cb.fit(tfidfv, y_train)\n",
    "predicted = cb.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, cb.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-gazette",
   "metadata": {},
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "skilled-mobility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.8058771148708815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.77      0.80      0.79       105\n",
      "           2       0.74      0.85      0.79        20\n",
      "           3       0.91      0.93      0.92       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.64      0.74      0.68        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.64      0.73      0.68        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.64      0.62      0.63        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.83      0.56      0.67         9\n",
      "          16       0.67      0.73      0.70        99\n",
      "          17       0.82      0.75      0.78        12\n",
      "          18       0.80      0.60      0.69        20\n",
      "          19       0.66      0.68      0.67       133\n",
      "          20       0.61      0.47      0.53        70\n",
      "          21       0.62      0.78      0.69        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.55      0.50      0.52        12\n",
      "          24       0.69      0.58      0.63        19\n",
      "          25       0.91      0.65      0.75        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.67      0.40      0.50        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       1.00      0.42      0.59        12\n",
      "          31       0.70      0.54      0.61        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.29      0.44         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.38      0.27      0.32        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.40      0.40      0.40         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.83      0.62      0.71         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.67      1.00      0.80         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.73      0.61      0.64      2246\n",
      "weighted avg       0.80      0.81      0.80      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=10000, penalty='l2')\n",
    "lr.fit(tfidfv, y_train)\n",
    "predicted = lr.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, lr.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-petroleum",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "disabled-income",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7666963490650045\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.67      0.70        12\n",
      "           1       0.72      0.71      0.72       105\n",
      "           2       0.74      0.70      0.72        20\n",
      "           3       0.90      0.91      0.90       813\n",
      "           4       0.79      0.83      0.81       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.80      0.86      0.83        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.56      0.66      0.60        38\n",
      "           9       0.73      0.76      0.75        25\n",
      "          10       0.74      0.77      0.75        30\n",
      "          11       0.60      0.73      0.66        83\n",
      "          12       0.23      0.23      0.23        13\n",
      "          13       0.59      0.54      0.56        37\n",
      "          14       0.33      0.50      0.40         2\n",
      "          15       0.50      0.22      0.31         9\n",
      "          16       0.65      0.71      0.68        99\n",
      "          17       0.80      0.33      0.47        12\n",
      "          18       0.85      0.55      0.67        20\n",
      "          19       0.64      0.63      0.63       133\n",
      "          20       0.53      0.51      0.52        70\n",
      "          21       0.58      0.70      0.63        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.56      0.42      0.48        12\n",
      "          24       0.53      0.47      0.50        19\n",
      "          25       0.84      0.52      0.64        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.27      0.30      0.29        10\n",
      "          29       0.38      0.75      0.50         4\n",
      "          30       0.78      0.58      0.67        12\n",
      "          31       0.71      0.38      0.50        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.80      0.57      0.67         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.44      0.36      0.40        11\n",
      "          37       0.40      1.00      0.57         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.60      0.30      0.40        10\n",
      "          41       0.62      0.62      0.62         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.75      1.00      0.86         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.65      0.57      0.58      2246\n",
      "weighted avg       0.77      0.77      0.76      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
    "lsvc.fit(tfidfv, y_train)\n",
    "predicted = lsvc.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, lsvc.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-geometry",
   "metadata": {},
   "source": [
    "### DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "serious-actor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6179875333926982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.72      0.40      0.52       105\n",
      "           2       0.60      0.45      0.51        20\n",
      "           3       0.94      0.84      0.89       813\n",
      "           4       0.39      0.91      0.55       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.57      0.73        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.88      0.88      0.88        25\n",
      "          10       0.87      0.87      0.87        30\n",
      "          11       0.62      0.48      0.54        83\n",
      "          12       0.17      0.08      0.11        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.60      0.82      0.69        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.62      0.26      0.37       133\n",
      "          20       0.33      0.03      0.05        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       1.00      0.05      0.10        19\n",
      "          25       0.86      0.19      0.32        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      2246\n",
      "   macro avg       0.24      0.17      0.18      2246\n",
      "weighted avg       0.61      0.62      0.57      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "tree.fit(tfidfv, y_train)\n",
    "predicted = tree.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, tree.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-observer",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "identified-twins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.701246660730187\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.42      0.33        12\n",
      "           1       0.42      0.78      0.55       105\n",
      "           2       0.44      0.35      0.39        20\n",
      "           3       0.84      0.90      0.87       813\n",
      "           4       0.68      0.84      0.75       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.86      0.43      0.57        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.59      0.53      0.56        38\n",
      "           9       0.71      0.40      0.51        25\n",
      "          10       0.89      0.53      0.67        30\n",
      "          11       0.57      0.69      0.62        83\n",
      "          12       0.33      0.15      0.21        13\n",
      "          13       0.46      0.32      0.38        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       1.00      0.11      0.20         9\n",
      "          16       0.70      0.67      0.68        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.60      0.45      0.51        20\n",
      "          19       0.62      0.64      0.63       133\n",
      "          20       0.46      0.33      0.38        70\n",
      "          21       0.65      0.41      0.50        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.75      0.25      0.38        12\n",
      "          24       0.33      0.05      0.09        19\n",
      "          25       0.87      0.42      0.57        31\n",
      "          26       1.00      0.12      0.22         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.33      0.25      0.29         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.30      0.46        10\n",
      "          33       1.00      0.20      0.33         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.33      0.09      0.14        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.25      0.12      0.17         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.33      0.50         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.70      2246\n",
      "   macro avg       0.54      0.31      0.36      2246\n",
      "weighted avg       0.69      0.70      0.68      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "forest.fit(tfidfv, y_train)\n",
    "predicted = forest.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, forest.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-clearing",
   "metadata": {},
   "source": [
    "### GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "protective-clinton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.767586821015138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.80      0.68      0.73       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.90      0.90      0.90       813\n",
      "           4       0.76      0.83      0.79       474\n",
      "           5       0.14      0.20      0.17         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.64      0.66      0.65        38\n",
      "           9       0.91      0.84      0.87        25\n",
      "          10       0.87      0.87      0.87        30\n",
      "          11       0.62      0.66      0.64        83\n",
      "          12       0.46      0.46      0.46        13\n",
      "          13       0.55      0.43      0.48        37\n",
      "          14       0.08      0.50      0.14         2\n",
      "          15       0.33      0.22      0.27         9\n",
      "          16       0.72      0.77      0.75        99\n",
      "          17       0.33      0.33      0.33        12\n",
      "          18       0.61      0.55      0.58        20\n",
      "          19       0.71      0.65      0.68       133\n",
      "          20       0.56      0.44      0.50        70\n",
      "          21       0.67      0.67      0.67        27\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.36      0.42      0.38        12\n",
      "          24       0.71      0.63      0.67        19\n",
      "          25       0.91      0.65      0.75        31\n",
      "          26       0.75      0.75      0.75         8\n",
      "          27       0.40      0.50      0.44         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.22      0.50      0.31         4\n",
      "          30       0.38      0.42      0.40        12\n",
      "          31       0.60      0.46      0.52        13\n",
      "          32       0.88      0.70      0.78        10\n",
      "          33       0.71      1.00      0.83         5\n",
      "          34       0.50      0.29      0.36         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.67      0.55      0.60        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.25      0.33      0.29         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.71      0.50      0.59        10\n",
      "          41       0.44      0.50      0.47         8\n",
      "          42       0.75      1.00      0.86         3\n",
      "          43       0.50      0.67      0.57         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.60      0.59      0.58      2246\n",
      "weighted avg       0.77      0.77      0.77      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grbt = GradientBoostingClassifier(random_state=0) # verbose=3\n",
    "grbt.fit(tfidfv, y_train)\n",
    "predicted = grbt.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, grbt.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-interference",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "little-jones",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.8161175422974176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.75      0.82        12\n",
      "           1       0.80      0.77      0.79       105\n",
      "           2       0.71      0.85      0.77        20\n",
      "           3       0.92      0.94      0.93       813\n",
      "           4       0.82      0.88      0.85       474\n",
      "           5       0.33      0.20      0.25         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       0.67      0.67      0.67         3\n",
      "           8       0.72      0.68      0.70        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.93      0.90      0.92        30\n",
      "          11       0.67      0.70      0.68        83\n",
      "          12       0.60      0.46      0.52        13\n",
      "          13       0.68      0.62      0.65        37\n",
      "          14       0.12      0.50      0.20         2\n",
      "          15       0.67      0.44      0.53         9\n",
      "          16       0.74      0.74      0.74        99\n",
      "          17       0.57      0.67      0.62        12\n",
      "          18       0.72      0.65      0.68        20\n",
      "          19       0.73      0.68      0.71       133\n",
      "          20       0.61      0.49      0.54        70\n",
      "          21       0.66      0.78      0.71        27\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.57      0.67      0.62        12\n",
      "          24       0.75      0.63      0.69        19\n",
      "          25       0.96      0.74      0.84        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       0.67      0.50      0.57         4\n",
      "          28       0.44      0.40      0.42        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       0.62      0.42      0.50        12\n",
      "          31       0.75      0.69      0.72        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.71      1.00      0.83         5\n",
      "          34       1.00      0.43      0.60         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.45      0.45      0.45        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.80      0.40      0.53        10\n",
      "          41       0.67      0.50      0.57         8\n",
      "          42       0.75      1.00      0.86         3\n",
      "          43       0.71      0.83      0.77         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.82      2246\n",
      "   macro avg       0.71      0.66      0.66      2246\n",
      "weighted avg       0.82      0.82      0.81      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voting_classifier = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "], voting='soft', n_jobs=-1)\n",
    "voting_classifier.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = voting_classifier.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "\n",
    "print(classification_report(y_test, voting_classifier.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-testing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "informed-andrews",
   "metadata": {},
   "source": [
    "# 3) num_words = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-stocks",
   "metadata": {},
   "source": [
    "# data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "numeric-scout",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=3000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-comparison",
   "metadata": {},
   "source": [
    "# data decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "roman-spectacular",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "\n",
    "index_to_word = {index + 3 : word for word, index in word_index.items()}\n",
    "\n",
    "# index_to_word에 숫자 0은 <pad>, 숫자 1은 <sos>, 숫자 2는 <unk>를 넣어줍니다.\n",
    "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    index_to_word[index]=token\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(x_train)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train = decoded\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(x_test)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test = decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-collar",
   "metadata": {},
   "source": [
    "# vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "united-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtmvector = CountVectorizer()\n",
    "x_train_dtm = dtmvector.fit_transform(x_train)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidfv = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "\n",
    "x_test_dtm = dtmvector.transform(x_test) #테스트 데이터를 DTM으로 변환\n",
    "tfidfv_test = tfidf_transformer.transform(x_test_dtm) #DTM을 TF-IDF 행렬로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-shame",
   "metadata": {},
   "source": [
    "# 8개 모델 테스트 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-sequence",
   "metadata": {},
   "source": [
    "###  NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "revolutionary-cleaning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6874443455031166\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.25      0.40        12\n",
      "           1       0.49      0.82      0.61       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.88      0.88      0.88       813\n",
      "           4       0.64      0.95      0.76       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.48      0.65        25\n",
      "          10       1.00      0.10      0.18        30\n",
      "          11       0.45      0.76      0.56        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.70      0.19      0.30        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.56      0.76      0.64        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.67      0.10      0.17        20\n",
      "          19       0.50      0.81      0.62       133\n",
      "          20       0.92      0.17      0.29        70\n",
      "          21       1.00      0.22      0.36        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.10      0.18        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.69      2246\n",
      "   macro avg       0.23      0.14      0.14      2246\n",
      "weighted avg       0.65      0.69      0.63      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "mod = MultinomialNB()\n",
    "mod.fit(tfidfv, y_train)\n",
    "predicted = mod.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, mod.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-publisher",
   "metadata": {},
   "source": [
    "### CNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "disabled-digest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7644701691896705\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.50      0.60        12\n",
      "           1       0.64      0.85      0.73       105\n",
      "           2       0.83      0.50      0.62        20\n",
      "           3       0.92      0.89      0.90       813\n",
      "           4       0.74      0.93      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.86      0.86      0.86        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.64      0.18      0.29        38\n",
      "           9       0.82      0.92      0.87        25\n",
      "          10       0.93      0.83      0.88        30\n",
      "          11       0.52      0.78      0.62        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.68      0.62      0.65        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.64      0.78      0.70        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.52      0.60      0.56        20\n",
      "          19       0.54      0.79      0.64       133\n",
      "          20       0.85      0.31      0.46        70\n",
      "          21       0.75      0.56      0.64        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.67      0.33      0.44        12\n",
      "          24       1.00      0.05      0.10        19\n",
      "          25       0.81      0.68      0.74        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.25      0.20      0.22        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       1.00      0.60      0.75         5\n",
      "          34       1.00      0.86      0.92         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       1.00      0.12      0.22         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.76      2246\n",
      "   macro avg       0.57      0.41      0.44      2246\n",
      "weighted avg       0.75      0.76      0.74      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cb = ComplementNB()\n",
    "cb.fit(tfidfv, y_train)\n",
    "predicted = cb.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, cb.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-victoria",
   "metadata": {},
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ceramic-yukon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.794746215494212\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.77      0.75      0.76       105\n",
      "           2       0.68      0.85      0.76        20\n",
      "           3       0.91      0.92      0.91       813\n",
      "           4       0.78      0.85      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.65      0.74      0.69        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.89      0.83      0.86        30\n",
      "          11       0.60      0.69      0.64        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.59      0.65      0.62        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.83      0.56      0.67         9\n",
      "          16       0.67      0.72      0.69        99\n",
      "          17       0.82      0.75      0.78        12\n",
      "          18       0.80      0.60      0.69        20\n",
      "          19       0.69      0.69      0.69       133\n",
      "          20       0.60      0.46      0.52        70\n",
      "          21       0.69      0.74      0.71        27\n",
      "          22       1.00      0.29      0.44         7\n",
      "          23       0.55      0.50      0.52        12\n",
      "          24       0.67      0.53      0.59        19\n",
      "          25       0.91      0.68      0.78        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.40      0.44        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       0.86      0.50      0.63        12\n",
      "          31       0.70      0.54      0.61        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.43      0.60         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.33      0.27      0.30        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.60      1.00      0.75         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.79      2246\n",
      "   macro avg       0.75      0.61      0.65      2246\n",
      "weighted avg       0.80      0.79      0.79      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=10000, penalty='l2')\n",
    "lr.fit(tfidfv, y_train)\n",
    "predicted = lr.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, lr.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-projector",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "british-cologne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7502226179875334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.58      0.67        12\n",
      "           1       0.64      0.71      0.68       105\n",
      "           2       0.75      0.75      0.75        20\n",
      "           3       0.88      0.88      0.88       813\n",
      "           4       0.78      0.81      0.80       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.88      1.00      0.93        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.59      0.63      0.61        38\n",
      "           9       0.76      0.76      0.76        25\n",
      "          10       0.82      0.77      0.79        30\n",
      "          11       0.61      0.69      0.64        83\n",
      "          12       0.55      0.46      0.50        13\n",
      "          13       0.62      0.62      0.62        37\n",
      "          14       1.00      0.50      0.67         2\n",
      "          15       0.83      0.56      0.67         9\n",
      "          16       0.60      0.71      0.65        99\n",
      "          17       0.50      0.17      0.25        12\n",
      "          18       0.83      0.50      0.62        20\n",
      "          19       0.63      0.62      0.62       133\n",
      "          20       0.50      0.44      0.47        70\n",
      "          21       0.53      0.67      0.59        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.43      0.50      0.46        12\n",
      "          24       0.54      0.37      0.44        19\n",
      "          25       0.84      0.68      0.75        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.30      0.37        10\n",
      "          29       0.14      0.25      0.18         4\n",
      "          30       0.67      0.33      0.44        12\n",
      "          31       0.50      0.31      0.38        13\n",
      "          32       0.89      0.80      0.84        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       0.57      0.57      0.57         7\n",
      "          35       0.50      0.17      0.25         6\n",
      "          36       0.43      0.55      0.48        11\n",
      "          37       0.40      1.00      0.57         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.29      0.40      0.33         5\n",
      "          40       0.50      0.30      0.37        10\n",
      "          41       0.33      0.25      0.29         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.75      1.00      0.86         6\n",
      "          44       0.60      0.60      0.60         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.75      2246\n",
      "   macro avg       0.63      0.55      0.56      2246\n",
      "weighted avg       0.75      0.75      0.75      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
    "lsvc.fit(tfidfv, y_train)\n",
    "predicted = lsvc.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, lsvc.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-possible",
   "metadata": {},
   "source": [
    "### DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "satisfied-advertiser",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6260017809439002\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.72      0.39      0.51       105\n",
      "           2       0.75      0.45      0.56        20\n",
      "           3       0.94      0.84      0.89       813\n",
      "           4       0.40      0.90      0.56       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.90      0.64      0.75        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.88      0.88      0.88        25\n",
      "          10       0.87      0.87      0.87        30\n",
      "          11       0.59      0.52      0.55        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.60      0.84      0.70        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.64      0.33      0.44       133\n",
      "          20       0.50      0.03      0.05        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.33      0.05      0.09        19\n",
      "          25       0.50      0.19      0.28        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       1.00      0.10      0.18        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       1.00      1.00      1.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.63      2246\n",
      "   macro avg       0.23      0.17      0.18      2246\n",
      "weighted avg       0.61      0.63      0.58      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "tree.fit(tfidfv, y_train)\n",
    "predicted = tree.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, tree.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-fleece",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "charming-catering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6856634016028496\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.58      0.40        12\n",
      "           1       0.44      0.72      0.55       105\n",
      "           2       0.12      0.10      0.11        20\n",
      "           3       0.85      0.91      0.88       813\n",
      "           4       0.66      0.83      0.73       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.67      0.57      0.62        14\n",
      "           7       0.33      0.33      0.33         3\n",
      "           8       0.54      0.53      0.53        38\n",
      "           9       0.69      0.44      0.54        25\n",
      "          10       0.72      0.43      0.54        30\n",
      "          11       0.54      0.59      0.56        83\n",
      "          12       0.50      0.15      0.24        13\n",
      "          13       0.41      0.32      0.36        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.61      0.52      0.56        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.55      0.30      0.39        20\n",
      "          19       0.56      0.53      0.55       133\n",
      "          20       0.57      0.37      0.45        70\n",
      "          21       0.68      0.56      0.61        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.80      0.21      0.33        19\n",
      "          25       0.93      0.42      0.58        31\n",
      "          26       1.00      0.12      0.22         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.20      0.33        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.33      0.18      0.24        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.67      0.20      0.31        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.33      0.17      0.22         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.69      2246\n",
      "   macro avg       0.41      0.29      0.31      2246\n",
      "weighted avg       0.66      0.69      0.66      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "forest.fit(tfidfv, y_train)\n",
    "predicted = forest.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, forest.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-whole",
   "metadata": {},
   "source": [
    "### GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "steady-pursuit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7756010685663401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.76      0.70      0.73       105\n",
      "           2       0.67      0.70      0.68        20\n",
      "           3       0.89      0.92      0.91       813\n",
      "           4       0.77      0.84      0.80       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       0.20      0.33      0.25         3\n",
      "           8       0.62      0.66      0.64        38\n",
      "           9       0.90      0.76      0.83        25\n",
      "          10       0.86      0.80      0.83        30\n",
      "          11       0.66      0.69      0.67        83\n",
      "          12       0.21      0.23      0.22        13\n",
      "          13       0.54      0.41      0.46        37\n",
      "          14       0.50      1.00      0.67         2\n",
      "          15       0.50      0.22      0.31         9\n",
      "          16       0.71      0.71      0.71        99\n",
      "          17       0.42      0.42      0.42        12\n",
      "          18       0.61      0.55      0.58        20\n",
      "          19       0.73      0.68      0.71       133\n",
      "          20       0.68      0.51      0.59        70\n",
      "          21       0.62      0.67      0.64        27\n",
      "          22       0.25      0.14      0.18         7\n",
      "          23       0.62      0.67      0.64        12\n",
      "          24       0.65      0.58      0.61        19\n",
      "          25       0.90      0.61      0.73        31\n",
      "          26       0.75      0.75      0.75         8\n",
      "          27       0.25      0.25      0.25         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.14      0.25      0.18         4\n",
      "          30       0.43      0.50      0.46        12\n",
      "          31       0.58      0.54      0.56        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.40      0.29      0.33         7\n",
      "          35       1.00      0.67      0.80         6\n",
      "          36       0.60      0.55      0.57        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.67      0.40      0.50        10\n",
      "          41       0.43      0.38      0.40         8\n",
      "          42       0.67      0.67      0.67         3\n",
      "          43       0.80      0.67      0.73         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.78      2246\n",
      "   macro avg       0.61      0.57      0.58      2246\n",
      "weighted avg       0.77      0.78      0.77      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grbt = GradientBoostingClassifier(random_state=0) # verbose=3\n",
    "grbt.fit(tfidfv, y_train)\n",
    "predicted = grbt.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, grbt.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-processing",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "previous-maine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.8103294746215495\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.77      0.78      0.77       105\n",
      "           2       0.73      0.80      0.76        20\n",
      "           3       0.92      0.94      0.93       813\n",
      "           4       0.82      0.88      0.85       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.68      0.74      0.71        38\n",
      "           9       0.80      0.80      0.80        25\n",
      "          10       0.90      0.87      0.88        30\n",
      "          11       0.65      0.70      0.67        83\n",
      "          12       0.50      0.38      0.43        13\n",
      "          13       0.62      0.62      0.62        37\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       0.67      0.44      0.53         9\n",
      "          16       0.70      0.73      0.71        99\n",
      "          17       0.62      0.67      0.64        12\n",
      "          18       0.76      0.65      0.70        20\n",
      "          19       0.71      0.71      0.71       133\n",
      "          20       0.58      0.44      0.50        70\n",
      "          21       0.66      0.78      0.71        27\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.64      0.75      0.69        12\n",
      "          24       0.65      0.58      0.61        19\n",
      "          25       0.92      0.71      0.80        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       0.50      0.25      0.33         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       0.67      0.50      0.57        12\n",
      "          31       0.64      0.69      0.67        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       0.75      0.43      0.55         7\n",
      "          35       1.00      0.67      0.80         6\n",
      "          36       0.45      0.45      0.45        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.20      0.20      0.20         5\n",
      "          40       0.60      0.30      0.40        10\n",
      "          41       0.75      0.38      0.50         8\n",
      "          42       0.67      0.67      0.67         3\n",
      "          43       0.71      0.83      0.77         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.70      0.63      0.65      2246\n",
      "weighted avg       0.81      0.81      0.81      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voting_classifier = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "], voting='soft', n_jobs=-1)\n",
    "voting_classifier.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = voting_classifier.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "\n",
    "print(classification_report(y_test, voting_classifier.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-founder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "documented-anthony",
   "metadata": {},
   "source": [
    "# 4) num_words = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-circumstances",
   "metadata": {},
   "source": [
    "# data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "incorrect-anthony",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=4000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-advocate",
   "metadata": {},
   "source": [
    "# data decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "comprehensive-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "\n",
    "index_to_word = {index + 3 : word for word, index in word_index.items()}\n",
    "\n",
    "# index_to_word에 숫자 0은 <pad>, 숫자 1은 <sos>, 숫자 2는 <unk>를 넣어줍니다.\n",
    "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    index_to_word[index]=token\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(x_train)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train = decoded\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(x_test)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test = decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-incident",
   "metadata": {},
   "source": [
    "# vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adjusted-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtmvector = CountVectorizer()\n",
    "x_train_dtm = dtmvector.fit_transform(x_train)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidfv = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "\n",
    "x_test_dtm = dtmvector.transform(x_test) #테스트 데이터를 DTM으로 변환\n",
    "tfidfv_test = tfidf_transformer.transform(x_test_dtm) #DTM을 TF-IDF 행렬로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-fossil",
   "metadata": {},
   "source": [
    "# 8개 모델 테스트 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-irrigation",
   "metadata": {},
   "source": [
    "###  NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "documentary-times",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6838824577025824\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.25      0.40        12\n",
      "           1       0.51      0.82      0.63       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.86      0.88      0.87       813\n",
      "           4       0.62      0.95      0.75       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.44      0.61        25\n",
      "          10       1.00      0.07      0.12        30\n",
      "          11       0.45      0.76      0.57        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       1.00      0.16      0.28        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.59      0.76      0.66        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.50      0.81      0.62       133\n",
      "          20       1.00      0.13      0.23        70\n",
      "          21       1.00      0.04      0.07        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.10      0.18        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.68      2246\n",
      "   macro avg       0.23      0.13      0.13      2246\n",
      "weighted avg       0.64      0.68      0.62      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "mod = MultinomialNB()\n",
    "mod.fit(tfidfv, y_train)\n",
    "predicted = mod.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, mod.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-berlin",
   "metadata": {},
   "source": [
    "### CNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "reflected-occurrence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7689225289403384\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.63      0.86      0.73       105\n",
      "           2       0.91      0.50      0.65        20\n",
      "           3       0.92      0.89      0.90       813\n",
      "           4       0.74      0.92      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.86      0.86      0.86        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.64      0.24      0.35        38\n",
      "           9       0.82      0.92      0.87        25\n",
      "          10       0.96      0.80      0.87        30\n",
      "          11       0.52      0.77      0.62        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.63      0.59      0.61        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.65      0.79      0.71        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.57      0.60      0.59        20\n",
      "          19       0.55      0.80      0.65       133\n",
      "          20       0.79      0.33      0.46        70\n",
      "          21       0.75      0.67      0.71        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.60      0.25      0.35        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       0.81      0.71      0.76        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.33      0.20      0.25        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.08      0.14        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.86      0.92         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.50      0.12      0.20         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.58      0.42      0.46      2246\n",
      "weighted avg       0.75      0.77      0.74      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cb = ComplementNB()\n",
    "cb.fit(tfidfv, y_train)\n",
    "predicted = cb.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, cb.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-vinyl",
   "metadata": {},
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "painful-belarus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7978628673196795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.78      0.78      0.78       105\n",
      "           2       0.70      0.80      0.74        20\n",
      "           3       0.91      0.93      0.92       813\n",
      "           4       0.79      0.86      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.66      0.71      0.68        38\n",
      "           9       0.82      0.92      0.87        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.63      0.71      0.67        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.62      0.65      0.63        37\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       0.83      0.56      0.67         9\n",
      "          16       0.67      0.71      0.69        99\n",
      "          17       0.73      0.67      0.70        12\n",
      "          18       0.80      0.60      0.69        20\n",
      "          19       0.69      0.68      0.69       133\n",
      "          20       0.57      0.44      0.50        70\n",
      "          21       0.62      0.78      0.69        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.55      0.50      0.52        12\n",
      "          24       0.65      0.58      0.61        19\n",
      "          25       0.91      0.68      0.78        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.40      0.44        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       0.80      0.33      0.47        12\n",
      "          31       0.67      0.46      0.55        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.75      0.43      0.55         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.33      0.27      0.30        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.60      1.00      0.75         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.80      2246\n",
      "   macro avg       0.73      0.61      0.64      2246\n",
      "weighted avg       0.79      0.80      0.79      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=10000, penalty='l2')\n",
    "lr.fit(tfidfv, y_train)\n",
    "predicted = lr.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, lr.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-establishment",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "jewish-singer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.767586821015138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.66      0.68      0.67       105\n",
      "           2       0.68      0.65      0.67        20\n",
      "           3       0.89      0.90      0.90       813\n",
      "           4       0.79      0.83      0.81       474\n",
      "           5       0.33      0.20      0.25         5\n",
      "           6       0.75      0.86      0.80        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.58      0.66      0.62        38\n",
      "           9       0.75      0.72      0.73        25\n",
      "          10       0.89      0.80      0.84        30\n",
      "          11       0.60      0.71      0.65        83\n",
      "          12       0.45      0.38      0.42        13\n",
      "          13       0.51      0.57      0.54        37\n",
      "          14       1.00      0.50      0.67         2\n",
      "          15       0.67      0.44      0.53         9\n",
      "          16       0.62      0.69      0.65        99\n",
      "          17       1.00      0.33      0.50        12\n",
      "          18       0.76      0.65      0.70        20\n",
      "          19       0.63      0.63      0.63       133\n",
      "          20       0.52      0.47      0.50        70\n",
      "          21       0.61      0.81      0.70        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.38      0.42      0.40        12\n",
      "          24       0.56      0.47      0.51        19\n",
      "          25       0.83      0.65      0.73        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.33      0.20      0.25        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       0.71      0.42      0.53        12\n",
      "          31       0.90      0.69      0.78        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.67      0.57      0.62         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.62      0.45      0.53        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.50      0.30      0.37        10\n",
      "          41       0.67      0.50      0.57         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.86      1.00      0.92         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.71      0.58      0.61      2246\n",
      "weighted avg       0.77      0.77      0.76      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
    "lsvc.fit(tfidfv, y_train)\n",
    "predicted = lsvc.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, lsvc.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-contributor",
   "metadata": {},
   "source": [
    "### DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "informational-logan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6202137132680321\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.71      0.40      0.51       105\n",
      "           2       0.50      0.35      0.41        20\n",
      "           3       0.94      0.85      0.89       813\n",
      "           4       0.40      0.91      0.55       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.64      0.78        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.79      0.88      0.83        25\n",
      "          10       0.87      0.87      0.87        30\n",
      "          11       0.66      0.48      0.56        83\n",
      "          12       0.17      0.08      0.11        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.60      0.80      0.68        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.61      0.26      0.37       133\n",
      "          20       0.40      0.03      0.05        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       1.00      0.05      0.10        19\n",
      "          25       0.75      0.19      0.31        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      2246\n",
      "   macro avg       0.20      0.15      0.15      2246\n",
      "weighted avg       0.61      0.62      0.57      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "tree.fit(tfidfv, y_train)\n",
    "predicted = tree.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, tree.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-knight",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "incorrect-barbados",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6878895814781835\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.50      0.44        12\n",
      "           1       0.36      0.70      0.47       105\n",
      "           2       0.32      0.30      0.31        20\n",
      "           3       0.84      0.90      0.87       813\n",
      "           4       0.69      0.85      0.76       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.82      0.64      0.72        14\n",
      "           7       0.25      0.33      0.29         3\n",
      "           8       0.51      0.53      0.52        38\n",
      "           9       0.53      0.36      0.43        25\n",
      "          10       0.59      0.33      0.43        30\n",
      "          11       0.59      0.61      0.60        83\n",
      "          12       0.33      0.15      0.21        13\n",
      "          13       0.30      0.19      0.23        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.59      0.58      0.58        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.70      0.35      0.47        20\n",
      "          19       0.61      0.56      0.59       133\n",
      "          20       0.54      0.37      0.44        70\n",
      "          21       0.78      0.26      0.39        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.20      0.08      0.12        12\n",
      "          24       0.80      0.21      0.33        19\n",
      "          25       0.93      0.45      0.61        31\n",
      "          26       1.00      0.12      0.22         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       1.00      0.10      0.18        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.50      0.08      0.13        13\n",
      "          32       1.00      0.30      0.46        10\n",
      "          33       1.00      0.60      0.75         5\n",
      "          34       0.25      0.14      0.18         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.71      0.45      0.56        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.50      0.17      0.25         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.69      2246\n",
      "   macro avg       0.47      0.29      0.33      2246\n",
      "weighted avg       0.68      0.69      0.66      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "forest.fit(tfidfv, y_train)\n",
    "predicted = forest.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, forest.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-cross",
   "metadata": {},
   "source": [
    "### GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "polish-toyota",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7671415850400712\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.76      0.68      0.71       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.89      0.91      0.90       813\n",
      "           4       0.77      0.84      0.80       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       0.67      0.67      0.67         3\n",
      "           8       0.68      0.68      0.68        38\n",
      "           9       0.83      0.80      0.82        25\n",
      "          10       0.80      0.80      0.80        30\n",
      "          11       0.64      0.67      0.65        83\n",
      "          12       0.35      0.46      0.40        13\n",
      "          13       0.56      0.41      0.47        37\n",
      "          14       0.20      0.50      0.29         2\n",
      "          15       0.40      0.22      0.29         9\n",
      "          16       0.69      0.73      0.71        99\n",
      "          17       0.40      0.33      0.36        12\n",
      "          18       0.61      0.55      0.58        20\n",
      "          19       0.69      0.66      0.67       133\n",
      "          20       0.57      0.44      0.50        70\n",
      "          21       0.68      0.70      0.69        27\n",
      "          22       0.33      0.14      0.20         7\n",
      "          23       0.53      0.67      0.59        12\n",
      "          24       0.57      0.42      0.48        19\n",
      "          25       0.86      0.61      0.72        31\n",
      "          26       0.75      0.75      0.75         8\n",
      "          27       0.25      0.25      0.25         4\n",
      "          28       0.30      0.30      0.30        10\n",
      "          29       0.29      0.50      0.36         4\n",
      "          30       0.36      0.42      0.38        12\n",
      "          31       0.64      0.54      0.58        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.50      0.29      0.36         7\n",
      "          35       1.00      0.67      0.80         6\n",
      "          36       0.57      0.36      0.44        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.25      0.33      0.29         3\n",
      "          39       0.50      0.40      0.44         5\n",
      "          40       0.40      0.20      0.27        10\n",
      "          41       0.45      0.62      0.53         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       0.50      0.67      0.57         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.58      0.52      0.54      2246\n",
      "weighted avg       0.76      0.77      0.76      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "grbt = GradientBoostingClassifier(random_state=0) # verbose=3\n",
    "grbt.fit(tfidfv, y_train)\n",
    "predicted = grbt.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, grbt.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-monroe",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "coordinated-speaking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.8130008904719501\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.75      0.82        12\n",
      "           1       0.79      0.80      0.80       105\n",
      "           2       0.68      0.85      0.76        20\n",
      "           3       0.92      0.94      0.93       813\n",
      "           4       0.83      0.87      0.85       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       0.67      0.67      0.67         3\n",
      "           8       0.72      0.68      0.70        38\n",
      "           9       0.75      0.84      0.79        25\n",
      "          10       0.90      0.90      0.90        30\n",
      "          11       0.66      0.70      0.68        83\n",
      "          12       0.43      0.46      0.44        13\n",
      "          13       0.66      0.62      0.64        37\n",
      "          14       0.33      0.50      0.40         2\n",
      "          15       0.57      0.44      0.50         9\n",
      "          16       0.72      0.74      0.73        99\n",
      "          17       0.58      0.58      0.58        12\n",
      "          18       0.72      0.65      0.68        20\n",
      "          19       0.73      0.68      0.71       133\n",
      "          20       0.58      0.46      0.51        70\n",
      "          21       0.61      0.81      0.70        27\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.64      0.75      0.69        12\n",
      "          24       0.75      0.63      0.69        19\n",
      "          25       0.88      0.71      0.79        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       0.50      0.25      0.33         4\n",
      "          28       0.40      0.40      0.40        10\n",
      "          29       0.40      0.50      0.44         4\n",
      "          30       0.56      0.42      0.48        12\n",
      "          31       0.69      0.69      0.69        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.67      0.29      0.40         7\n",
      "          35       1.00      0.67      0.80         6\n",
      "          36       0.50      0.45      0.48        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.60      0.60      0.60         5\n",
      "          40       0.67      0.20      0.31        10\n",
      "          41       0.62      0.62      0.62         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.71      0.83      0.77         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.66      0.60      0.62      2246\n",
      "weighted avg       0.81      0.81      0.81      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "voting_classifier = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "], voting='soft', n_jobs=-1)\n",
    "voting_classifier.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = voting_classifier.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "\n",
    "print(classification_report(y_test, voting_classifier.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-brother",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-target",
   "metadata": {},
   "source": [
    "# 5) num_words = 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-process",
   "metadata": {},
   "source": [
    "# data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "amended-civilian",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=8000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-emission",
   "metadata": {},
   "source": [
    "# data decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "nervous-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "\n",
    "index_to_word = {index + 3 : word for word, index in word_index.items()}\n",
    "\n",
    "# index_to_word에 숫자 0은 <pad>, 숫자 1은 <sos>, 숫자 2는 <unk>를 넣어줍니다.\n",
    "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    index_to_word[index]=token\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(x_train)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train = decoded\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(x_test)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test = decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-pontiac",
   "metadata": {},
   "source": [
    "# vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dedicated-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtmvector = CountVectorizer()\n",
    "x_train_dtm = dtmvector.fit_transform(x_train)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidfv = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "\n",
    "x_test_dtm = dtmvector.transform(x_test) #테스트 데이터를 DTM으로 변환\n",
    "tfidfv_test = tfidf_transformer.transform(x_test_dtm) #DTM을 TF-IDF 행렬로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-chicken",
   "metadata": {},
   "source": [
    "# 8개 모델 테스트 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-carpet",
   "metadata": {},
   "source": [
    "###  NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "hundred-groove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6625111308993766\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.59      0.75      0.66       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.82      0.90      0.86       813\n",
      "           4       0.54      0.96      0.69       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.16      0.28        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.58      0.67      0.63        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       1.00      0.05      0.10        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.66      0.59      0.62        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.55      0.78      0.64       133\n",
      "          20       1.00      0.04      0.08        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.03      0.06        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.66      2246\n",
      "   macro avg       0.17      0.11      0.10      2246\n",
      "weighted avg       0.59      0.66      0.58      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "mod = MultinomialNB()\n",
    "mod.fit(tfidfv, y_train)\n",
    "predicted = mod.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, mod.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-frequency",
   "metadata": {},
   "source": [
    "### CNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "built-storm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7707034728406055\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73        12\n",
      "           1       0.64      0.87      0.74       105\n",
      "           2       0.91      0.50      0.65        20\n",
      "           3       0.91      0.89      0.90       813\n",
      "           4       0.74      0.93      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.87      0.93      0.90        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.64      0.24      0.35        38\n",
      "           9       0.79      0.92      0.85        25\n",
      "          10       0.96      0.80      0.87        30\n",
      "          11       0.55      0.73      0.63        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.63      0.59      0.61        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.66      0.78      0.72        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.57      0.60      0.59        20\n",
      "          19       0.55      0.80      0.66       133\n",
      "          20       0.78      0.30      0.43        70\n",
      "          21       0.75      0.67      0.71        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.71      0.42      0.53        12\n",
      "          24       0.50      0.11      0.17        19\n",
      "          25       0.88      0.71      0.79        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.29      0.20      0.24        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.15      0.27        13\n",
      "          32       1.00      0.60      0.75        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.66      0.45      0.49      2246\n",
      "weighted avg       0.76      0.77      0.75      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cb = ComplementNB()\n",
    "cb.fit(tfidfv, y_train)\n",
    "predicted = cb.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, cb.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-james",
   "metadata": {},
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "incorporated-dubai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.8098842386464826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.75      0.78      0.77       105\n",
      "           2       0.74      0.85      0.79        20\n",
      "           3       0.92      0.94      0.93       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.69      0.71      0.70        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.93      0.90      0.92        30\n",
      "          11       0.67      0.73      0.70        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.61      0.62      0.61        37\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       0.83      0.56      0.67         9\n",
      "          16       0.68      0.74      0.71        99\n",
      "          17       0.69      0.75      0.72        12\n",
      "          18       0.76      0.65      0.70        20\n",
      "          19       0.68      0.67      0.67       133\n",
      "          20       0.60      0.50      0.55        70\n",
      "          21       0.63      0.81      0.71        27\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.70      0.58      0.64        12\n",
      "          24       0.67      0.53      0.59        19\n",
      "          25       0.91      0.68      0.78        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.57      0.40      0.47        10\n",
      "          29       0.57      1.00      0.73         4\n",
      "          30       0.83      0.42      0.56        12\n",
      "          31       0.75      0.46      0.57        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.75      0.43      0.55         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.30      0.27      0.29        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.67      0.40      0.50         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.83      0.62      0.71         8\n",
      "          42       1.00      1.00      1.00         3\n",
      "          43       0.75      1.00      0.86         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.77      0.64      0.67      2246\n",
      "weighted avg       0.81      0.81      0.80      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=10000, penalty='l2')\n",
    "lr.fit(tfidfv, y_train)\n",
    "predicted = lr.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, lr.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-alabama",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "returning-poland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.776046304541407\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.69      0.70      0.70       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.91      0.92      0.91       813\n",
      "           4       0.81      0.83      0.82       474\n",
      "           5       0.67      0.40      0.50         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.59      0.68      0.63        38\n",
      "           9       0.78      0.84      0.81        25\n",
      "          10       0.92      0.80      0.86        30\n",
      "          11       0.62      0.72      0.67        83\n",
      "          12       0.36      0.31      0.33        13\n",
      "          13       0.62      0.57      0.59        37\n",
      "          14       0.14      0.50      0.22         2\n",
      "          15       0.71      0.56      0.63         9\n",
      "          16       0.64      0.71      0.67        99\n",
      "          17       0.50      0.17      0.25        12\n",
      "          18       0.75      0.60      0.67        20\n",
      "          19       0.59      0.61      0.60       133\n",
      "          20       0.54      0.54      0.54        70\n",
      "          21       0.54      0.81      0.65        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.67      0.67      0.67        12\n",
      "          24       0.69      0.47      0.56        19\n",
      "          25       0.83      0.61      0.70        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.50      0.67         4\n",
      "          28       0.60      0.30      0.40        10\n",
      "          29       0.25      0.50      0.33         4\n",
      "          30       0.57      0.33      0.42        12\n",
      "          31       0.71      0.38      0.50        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.67      0.57      0.62         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.67      0.55      0.60        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.33      0.33      0.33         3\n",
      "          39       0.40      0.40      0.40         5\n",
      "          40       0.50      0.30      0.37        10\n",
      "          41       0.60      0.38      0.46         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.86      1.00      0.92         6\n",
      "          44       0.80      0.80      0.80         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.78      2246\n",
      "   macro avg       0.67      0.59      0.61      2246\n",
      "weighted avg       0.78      0.78      0.77      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
    "lsvc.fit(tfidfv, y_train)\n",
    "predicted = lsvc.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, lsvc.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-singles",
   "metadata": {},
   "source": [
    "### DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "satisfied-wallpaper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6206589492430988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.72      0.41      0.52       105\n",
      "           2       0.75      0.45      0.56        20\n",
      "           3       0.93      0.84      0.88       813\n",
      "           4       0.40      0.91      0.56       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.90      0.64      0.75        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.88      0.88      0.88        25\n",
      "          10       0.86      0.83      0.85        30\n",
      "          11       0.66      0.45      0.53        83\n",
      "          12       0.14      0.08      0.10        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.61      0.84      0.71        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.61      0.28      0.38       133\n",
      "          20       0.67      0.03      0.05        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.33      0.05      0.09        19\n",
      "          25       0.55      0.19      0.29        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      2246\n",
      "   macro avg       0.21      0.17      0.18      2246\n",
      "weighted avg       0.61      0.62      0.57      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "tree.fit(tfidfv, y_train)\n",
    "predicted = tree.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, tree.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-vegetarian",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "solved-variance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6669634906500446\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.50      0.48        12\n",
      "           1       0.36      0.66      0.46       105\n",
      "           2       0.21      0.20      0.21        20\n",
      "           3       0.82      0.90      0.86       813\n",
      "           4       0.65      0.83      0.73       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.50      0.43      0.46        14\n",
      "           7       0.20      0.33      0.25         3\n",
      "           8       0.54      0.55      0.55        38\n",
      "           9       0.38      0.20      0.26        25\n",
      "          10       0.73      0.27      0.39        30\n",
      "          11       0.48      0.54      0.51        83\n",
      "          12       0.33      0.15      0.21        13\n",
      "          13       0.38      0.24      0.30        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.61      0.49      0.55        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.45      0.25      0.32        20\n",
      "          19       0.63      0.57      0.60       133\n",
      "          20       0.48      0.33      0.39        70\n",
      "          21       0.80      0.30      0.43        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.60      0.16      0.25        19\n",
      "          25       1.00      0.42      0.59        31\n",
      "          26       1.00      0.12      0.22         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.10      0.18        10\n",
      "          33       1.00      0.20      0.33         5\n",
      "          34       0.50      0.14      0.22         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.50      0.18      0.27        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.60      0.50      0.55         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.67      2246\n",
      "   macro avg       0.42      0.24      0.27      2246\n",
      "weighted avg       0.65      0.67      0.64      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "forest.fit(tfidfv, y_train)\n",
    "predicted = forest.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, forest.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-bottom",
   "metadata": {},
   "source": [
    "### GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "numeric-methodology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7644701691896705\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.75      0.72        12\n",
      "           1       0.77      0.67      0.71       105\n",
      "           2       0.68      0.75      0.71        20\n",
      "           3       0.88      0.90      0.89       813\n",
      "           4       0.75      0.85      0.80       474\n",
      "           5       0.50      0.20      0.29         5\n",
      "           6       0.77      0.71      0.74        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.61      0.66      0.63        38\n",
      "           9       0.91      0.80      0.85        25\n",
      "          10       0.90      0.87      0.88        30\n",
      "          11       0.64      0.69      0.66        83\n",
      "          12       0.29      0.31      0.30        13\n",
      "          13       0.65      0.46      0.54        37\n",
      "          14       0.10      0.50      0.17         2\n",
      "          15       0.33      0.11      0.17         9\n",
      "          16       0.74      0.76      0.75        99\n",
      "          17       0.60      0.25      0.35        12\n",
      "          18       0.50      0.45      0.47        20\n",
      "          19       0.71      0.65      0.68       133\n",
      "          20       0.61      0.40      0.48        70\n",
      "          21       0.61      0.52      0.56        27\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.58      0.58      0.58        12\n",
      "          24       0.53      0.53      0.53        19\n",
      "          25       0.92      0.71      0.80        31\n",
      "          26       0.80      1.00      0.89         8\n",
      "          27       0.29      0.50      0.36         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.33      0.75      0.46         4\n",
      "          30       0.36      0.42      0.38        12\n",
      "          31       0.55      0.46      0.50        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.67      0.80      0.73         5\n",
      "          34       0.43      0.43      0.43         7\n",
      "          35       1.00      0.67      0.80         6\n",
      "          36       0.50      0.45      0.48        11\n",
      "          37       0.33      0.50      0.40         2\n",
      "          38       0.33      0.33      0.33         3\n",
      "          39       0.33      0.20      0.25         5\n",
      "          40       0.60      0.30      0.40        10\n",
      "          41       0.36      0.50      0.42         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.44      0.67      0.53         6\n",
      "          44       0.80      0.80      0.80         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.76      2246\n",
      "   macro avg       0.61      0.57      0.57      2246\n",
      "weighted avg       0.76      0.76      0.76      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grbt = GradientBoostingClassifier(random_state=0) # verbose=3\n",
    "grbt.fit(tfidfv, y_train)\n",
    "predicted = grbt.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, grbt.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-basket",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "federal-thumb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.813446126447017\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78        12\n",
      "           1       0.77      0.74      0.76       105\n",
      "           2       0.69      0.90      0.78        20\n",
      "           3       0.93      0.94      0.93       813\n",
      "           4       0.82      0.88      0.85       474\n",
      "           5       0.50      0.20      0.29         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.70      0.68      0.69        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.90      0.90      0.90        30\n",
      "          11       0.65      0.70      0.67        83\n",
      "          12       0.44      0.31      0.36        13\n",
      "          13       0.66      0.68      0.67        37\n",
      "          14       0.14      0.50      0.22         2\n",
      "          15       0.67      0.44      0.53         9\n",
      "          16       0.75      0.75      0.75        99\n",
      "          17       0.80      0.67      0.73        12\n",
      "          18       0.79      0.55      0.65        20\n",
      "          19       0.72      0.68      0.70       133\n",
      "          20       0.58      0.47      0.52        70\n",
      "          21       0.66      0.78      0.71        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.62      0.67      0.64        12\n",
      "          24       0.67      0.63      0.65        19\n",
      "          25       0.92      0.74      0.82        31\n",
      "          26       0.89      1.00      0.94         8\n",
      "          27       0.33      0.50      0.40         4\n",
      "          28       0.44      0.40      0.42        10\n",
      "          29       0.43      0.75      0.55         4\n",
      "          30       0.67      0.50      0.57        12\n",
      "          31       0.82      0.69      0.75        13\n",
      "          32       1.00      0.90      0.95        10\n",
      "          33       0.71      1.00      0.83         5\n",
      "          34       0.57      0.57      0.57         7\n",
      "          35       1.00      0.67      0.80         6\n",
      "          36       0.45      0.45      0.45        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.57      0.50      0.53         8\n",
      "          42       1.00      1.00      1.00         3\n",
      "          43       0.67      1.00      0.80         6\n",
      "          44       0.80      0.80      0.80         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.71      0.65      0.65      2246\n",
      "weighted avg       0.81      0.81      0.81      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voting_classifier = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "], voting='soft', n_jobs=-1)\n",
    "voting_classifier.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = voting_classifier.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "\n",
    "print(classification_report(y_test, voting_classifier.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-pickup",
   "metadata": {},
   "source": [
    "## 최종 결과 표\n",
    "| num\\model | NB | CNB | LR | SVM | DT | RF | GBT | Ensemble |\n",
    "|:------:|------|------|------|------|------|------|------|-------|\n",
    "| None  | 0.59 | 0.76 | 0.81 | 0.77 | 0.62 | 0.65 | 0.77 | 0.818 |\n",
    "| 10000 | 0.65 | 0.77 | 0.80 | 0.78 | 0.62 | 0.67 | 0.76 | 0.811 |\n",
    "| 8000 | 0.66 | 0.77 | 0.80 | 0.77 | 0.62 | 0.66 | 0.76 | 0.813 |\n",
    "| 5000 | 0.67 | 0.77 | 0.80 | 0.76 | 0.61 | 0.70 | 0.76 | 0.816 |\n",
    "| 4000 | 0.68 | 0.76 | 0.79 | 0.76 | 0.62 | 0.68 | 0.76 | 0.813 | \n",
    "| 3000 | 0.68 | 0.76 | 0.79 | 0.75 | 0.62 | 0.68 | 0.77 | 0.810 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-activation",
   "metadata": {},
   "source": [
    "## 각 모델에서 num_words와 accuracy의 관계 비교\n",
    "NB 는 num_words가 클수록 accuracy는 낮아짐      \n",
    "RF 는 num_words가 클수록 accuracy는 낮아짐      \n",
    "   \n",
    "   \n",
    "   \n",
    "LR 은 num_words가 클수록 accuracy는 높아짐      \n",
    "SVM은 num_words가 클수록 accuracy는 높아짐 (None에선 다시 떨어짐)     \n",
    "   \n",
    "   \n",
    "    \n",
    "    \n",
    "DT 는 num_words 크기와 accuracy사이 관계 없음   \n",
    "CNB는 num_words 크기와 accuracy사이 관계 없음   \n",
    "GBT는 num_words 크기와 accuracy사이 관계 없음   \n",
    "Ensemble은 num_words 크기와 accuracy사이 관계 없음   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-gentleman",
   "metadata": {},
   "source": [
    "# 분석   \n",
    "사실 num_words에 따라 accuracy가 크게 차이나는 모델들은 아래와 같다.   \n",
    "   \n",
    "   \n",
    "   \n",
    "LR 로지스틱 회귀는 2% 내외,   \n",
    "SVM 서포트 벡터 머신은 3% 내외,   \n",
    "RF 랜덤 포레스트는 5% 내외,   \n",
    "NB 나이브 베이즈 분류기는 9% 내외의 차이를 보였다.   \n",
    "   \n",
    "   \n",
    "가장 성능이 좋은 건 Ensemble로 num_words개수에 크게 영향 받지 않고 모두 81% 대 accuracy를 보였다.   \n",
    "   \n",
    "ensemble 중 accuracy가 그나마 제일 높은 건 num_words가 none일 때였다.   \n",
    "### num_words = None, Ensemble(CNB, LR, CBT) accuracy = 0.818"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-technical",
   "metadata": {},
   "source": [
    "# 2. 딥러닝 모델과 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "technical-telescope",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words= 5000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "honey-cheat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 2376\n",
      "리뷰의 평균 길이 : 145.5398574927633\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train 데이터셋 내 문장 길이 분포  \n",
    "print('리뷰의 최대 길이 :',max(len(i) for i in x_train))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, x_train))/len(x_train))\n",
    "plt.hist([len(s) for s in x_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "early-rally",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 1032\n",
      "리뷰의 평균 길이 : 147.66117542297417\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train 데이터셋 내 문장 길이 분포  \n",
    "print('리뷰의 최대 길이 :',max(len(i) for i in x_test))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, x_test))/len(x_test))\n",
    "plt.hist([len(s) for s in x_test], bins=100)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cultural-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, padding='post', maxlen = maxlen)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, padding='post', maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "constitutional-hamburg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "upset-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train) # 훈련용 뉴스 기사 레이블의 원-핫 인코딩\n",
    "y_test = to_categorical(y_test) # 테스트용 뉴스 기사 레이블의 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "obvious-consultancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "theoretical-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  LSTM으로 학습\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(5000, 120),# (Size of the vocabulary, Dimension of the dense embedding)\n",
    "    keras.layers.LSTM(120),\n",
    "    keras.layers.Dense(120, activation='relu'),\n",
    "    keras.layers.Dense(46, activation='softmax')\n",
    "])\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# model compile\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "better-pregnancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "223/225 [============================>.] - ETA: 0s - loss: 2.4737 - acc: 0.3526\n",
      "Epoch 00001: val_acc improved from -inf to 0.35782, saving model to best_model.h5\n",
      "225/225 [==============================] - 4s 18ms/step - loss: 2.4751 - acc: 0.3523 - val_loss: 2.3581 - val_acc: 0.3578\n",
      "Epoch 2/100\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.3302 - acc: 0.3701\n",
      "Epoch 00002: val_acc improved from 0.35782 to 0.41291, saving model to best_model.h5\n",
      "225/225 [==============================] - 4s 17ms/step - loss: 2.3297 - acc: 0.3701 - val_loss: 2.1676 - val_acc: 0.4129\n",
      "Epoch 3/100\n",
      "223/225 [============================>.] - ETA: 0s - loss: 2.1588 - acc: 0.3908\n",
      "Epoch 00003: val_acc did not improve from 0.41291\n",
      "225/225 [==============================] - 4s 18ms/step - loss: 2.1567 - acc: 0.3901 - val_loss: 2.1421 - val_acc: 0.3934\n",
      "Epoch 4/100\n",
      "223/225 [============================>.] - ETA: 0s - loss: 2.0731 - acc: 0.4026\n",
      "Epoch 00004: val_acc did not improve from 0.41291\n",
      "225/225 [==============================] - 4s 18ms/step - loss: 2.0728 - acc: 0.4028 - val_loss: 2.0611 - val_acc: 0.3923\n",
      "Epoch 5/100\n",
      "222/225 [============================>.] - ETA: 0s - loss: 2.2801 - acc: 0.3784\n",
      "Epoch 00005: val_acc did not improve from 0.41291\n",
      "225/225 [==============================] - 4s 18ms/step - loss: 2.2841 - acc: 0.3772 - val_loss: 2.3545 - val_acc: 0.3550\n",
      "Epoch 6/100\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.3222 - acc: 0.3673\n",
      "Epoch 00006: val_acc did not improve from 0.41291\n",
      "225/225 [==============================] - 4s 17ms/step - loss: 2.3222 - acc: 0.3673 - val_loss: 2.3682 - val_acc: 0.3595\n",
      "Epoch 7/100\n",
      "223/225 [============================>.] - ETA: 0s - loss: 2.1132 - acc: 0.3906\n",
      "Epoch 00007: val_acc did not improve from 0.41291\n",
      "225/225 [==============================] - 4s 17ms/step - loss: 2.1125 - acc: 0.3900 - val_loss: 1.9690 - val_acc: 0.4001\n",
      "Epoch 8/100\n",
      "225/225 [==============================] - ETA: 0s - loss: 1.8983 - acc: 0.4177\n",
      "Epoch 00008: val_acc improved from 0.41291 to 0.42515, saving model to best_model.h5\n",
      "225/225 [==============================] - 4s 17ms/step - loss: 1.8983 - acc: 0.4177 - val_loss: 1.9169 - val_acc: 0.4252\n",
      "Epoch 9/100\n",
      "225/225 [==============================] - ETA: 0s - loss: 1.7528 - acc: 0.5170\n",
      "Epoch 00009: val_acc improved from 0.42515 to 0.56483, saving model to best_model.h5\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 1.7528 - acc: 0.5170 - val_loss: 1.6920 - val_acc: 0.5648\n",
      "Epoch 10/100\n",
      "224/225 [============================>.] - ETA: 0s - loss: 1.6116 - acc: 0.5806\n",
      "Epoch 00010: val_acc improved from 0.56483 to 0.58264, saving model to best_model.h5\n",
      "225/225 [==============================] - 4s 18ms/step - loss: 1.6108 - acc: 0.5808 - val_loss: 1.6301 - val_acc: 0.5826\n",
      "Epoch 11/100\n",
      "223/225 [============================>.] - ETA: 0s - loss: 1.5271 - acc: 0.5971\n",
      "Epoch 00011: val_acc improved from 0.58264 to 0.58319, saving model to best_model.h5\n",
      "225/225 [==============================] - 4s 18ms/step - loss: 1.5260 - acc: 0.5971 - val_loss: 1.6130 - val_acc: 0.5832\n",
      "Epoch 12/100\n",
      "222/225 [============================>.] - ETA: 0s - loss: 1.4439 - acc: 0.6129\n",
      "Epoch 00012: val_acc did not improve from 0.58319\n",
      "225/225 [==============================] - 4s 18ms/step - loss: 1.4480 - acc: 0.6122 - val_loss: 1.5663 - val_acc: 0.5804\n",
      "Epoch 13/100\n",
      "223/225 [============================>.] - ETA: 0s - loss: 1.3550 - acc: 0.6449\n",
      "Epoch 00013: val_acc improved from 0.58319 to 0.65220, saving model to best_model.h5\n",
      "225/225 [==============================] - 4s 18ms/step - loss: 1.3538 - acc: 0.6455 - val_loss: 1.4626 - val_acc: 0.6522\n",
      "Epoch 14/100\n",
      "225/225 [==============================] - ETA: 0s - loss: 1.2170 - acc: 0.6965\n",
      "Epoch 00014: val_acc improved from 0.65220 to 0.67168, saving model to best_model.h5\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 1.2170 - acc: 0.6965 - val_loss: 1.3822 - val_acc: 0.6717\n",
      "Epoch 15/100\n",
      "223/225 [============================>.] - ETA: 0s - loss: 1.0957 - acc: 0.7305\n",
      "Epoch 00015: val_acc improved from 0.67168 to 0.67557, saving model to best_model.h5\n",
      "225/225 [==============================] - 4s 18ms/step - loss: 1.0952 - acc: 0.7304 - val_loss: 1.3808 - val_acc: 0.6756\n",
      "Epoch 16/100\n",
      "224/225 [============================>.] - ETA: 0s - loss: 1.0013 - acc: 0.7506\n",
      "Epoch 00016: val_acc improved from 0.67557 to 0.69060, saving model to best_model.h5\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 1.0034 - acc: 0.7502 - val_loss: 1.3149 - val_acc: 0.6906\n",
      "Epoch 17/100\n",
      "223/225 [============================>.] - ETA: 0s - loss: 0.8996 - acc: 0.7742\n",
      "Epoch 00017: val_acc did not improve from 0.69060\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.8990 - acc: 0.7743 - val_loss: 1.3733 - val_acc: 0.6717\n",
      "Epoch 18/100\n",
      "222/225 [============================>.] - ETA: 0s - loss: 0.8344 - acc: 0.7879\n",
      "Epoch 00018: val_acc did not improve from 0.69060\n",
      "225/225 [==============================] - 4s 20ms/step - loss: 0.8366 - acc: 0.7873 - val_loss: 1.3770 - val_acc: 0.6867\n",
      "Epoch 19/100\n",
      "224/225 [============================>.] - ETA: 0s - loss: 0.7699 - acc: 0.8026\n",
      "Epoch 00019: val_acc improved from 0.69060 to 0.69171, saving model to best_model.h5\n",
      "225/225 [==============================] - 4s 18ms/step - loss: 0.7701 - acc: 0.8028 - val_loss: 1.3831 - val_acc: 0.6917\n",
      "Epoch 20/100\n",
      "222/225 [============================>.] - ETA: 0s - loss: 0.7007 - acc: 0.8240\n",
      "Epoch 00020: val_acc improved from 0.69171 to 0.70061, saving model to best_model.h5\n",
      "225/225 [==============================] - 4s 18ms/step - loss: 0.7019 - acc: 0.8238 - val_loss: 1.3995 - val_acc: 0.7006\n",
      "Epoch 21/100\n",
      "224/225 [============================>.] - ETA: 0s - loss: 0.6484 - acc: 0.8373\n",
      "Epoch 00021: val_acc improved from 0.70061 to 0.70562, saving model to best_model.h5\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.6484 - acc: 0.8373 - val_loss: 1.4140 - val_acc: 0.7056\n",
      "Epoch 22/100\n",
      "222/225 [============================>.] - ETA: 0s - loss: 0.5892 - acc: 0.8519\n",
      "Epoch 00022: val_acc did not improve from 0.70562\n",
      "225/225 [==============================] - 4s 18ms/step - loss: 0.5896 - acc: 0.8522 - val_loss: 1.4759 - val_acc: 0.6995\n",
      "Epoch 23/100\n",
      "222/225 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.8674\n",
      "Epoch 00023: val_acc did not improve from 0.70562\n",
      "225/225 [==============================] - 4s 18ms/step - loss: 0.5364 - acc: 0.8678 - val_loss: 1.5112 - val_acc: 0.6967\n",
      "Epoch 24/100\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.5005 - acc: 0.8772\n",
      "Epoch 00024: val_acc did not improve from 0.70562\n",
      "225/225 [==============================] - 4s 18ms/step - loss: 0.5005 - acc: 0.8772 - val_loss: 1.5504 - val_acc: 0.7040\n",
      "Epoch 25/100\n",
      "222/225 [============================>.] - ETA: 0s - loss: 0.4619 - acc: 0.8861\n",
      "Epoch 00025: val_acc improved from 0.70562 to 0.70785, saving model to best_model.h5\n",
      "225/225 [==============================] - 4s 18ms/step - loss: 0.4602 - acc: 0.8864 - val_loss: 1.5682 - val_acc: 0.7078\n",
      "Epoch 26/100\n",
      "222/225 [============================>.] - ETA: 0s - loss: 0.4310 - acc: 0.8956\n",
      "Epoch 00026: val_acc did not improve from 0.70785\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.4287 - acc: 0.8963 - val_loss: 1.6019 - val_acc: 0.6939\n",
      "Epoch 00026: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, callbacks=[early_stopping, model_checkpoint], \n",
    "                   batch_size=32 ,validation_split=0.2\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "skilled-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "religious-honduras",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "epochs = range(1, len(loss)+1)\n",
    "\n",
    "plt.plot(epochs ,loss, label='Training loss')\n",
    "plt.plot(epochs ,val_loss,label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "handled-library",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 0s 7ms/step - loss: 1.6513 - acc: 0.7044\n",
      "\n",
      " 테스트 정확도: 0.7044\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-stone",
   "metadata": {},
   "source": [
    "# 딥러닝 모델의 test accuracy가 10% 가량 더 낮다. 왜?   \n",
    "### 데이터 개수가 적어서 인듯 (훈련 셋만 9000개 미만)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-place",
   "metadata": {},
   "source": [
    "# 회고   \n",
    "### 1. (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words= None, test_split=0.2) 이렇게 데이터 로드 시 num_words를 None으로 줬을 때,     keras.layers.Embedding(None, 100)  임베딩 레이어의 vocab_size는 None으로 줄 수가 없는 건가?  None으로 주면 계속 에러나서  (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words= 5000, test_split=0.2), keras.layers.Embedding(5000, 100) 전부 5000으로 맞춰야했다.   \n",
    "   \n",
    "      \n",
    "### 2. 로이터 뉴스데이터를 로드했을 때 y_train을 곧바로 모델 입력값으로 넣으면 에러가 났다. from tensorflow.keras.utils import to_categorical 이렇게 to_categorical 이란 모듈을 import해준 다음 y_train = to_categorical(y_train) 이렇게 레이블을 원핫인코딩으로 바꿔줘야 모델이 훈련이 됐다. 왜?   \n",
    "   \n",
    "   \n",
    "### 3. 모델 파라미터들, keras.layers.Embedding(5000, 120)에서 임베딩 벡터의 차원 수 120, keras.layers.LSTM(120)에서 lstm 레이어의 유닛수 120,  keras.layers.Dense(120, activation='relu')에서 덴스 레이어의 유닛수 120,  이 파라미터들을 너무 작게 하면 (16, 8, 8) 언더피팅?이 된다. (훈련 정확도가 40%를 못넘는다) 이들 파라미터는 뭘 보고 맞추는 거지? 그냥 내가 했던 것처럼 무식하게 여러 번 조합해서 넣어봐야 되나? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-tumor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
